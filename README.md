# adventureworks_etl
End-to-End Azure ETL Project â€“ AdventureWorks

ğŸ“Œ Project Overview

This project demonstrates a production-grade ETL pipeline built using Microsoft Azure services and SQL Server.

The solution extracts transactional sales data from SQL Server, applies business transformations, and loads curated data into Azure Databricks Hive tables under Unity Catalog for analytics and reporting.


---

ğŸ—ï¸ Architecture Overview

SQL Server (AdventureWorks)
        â†“
Azure Data Factory (Orchestration)
        â†“
Azure Data Lake Storage Gen2
        â†“
Azure Databricks (Transformation & Load)
        â†“
Unity Catalog Managed Delta Tables
        â†“
SQL Logging + Email Alerts


---

ğŸ—„ï¸ Source System

SQL Server (On-Prem)

AdventureWorksLT2025 Database

Tables Used:

SalesOrderHeader

SalesOrderDetail

Customer

Product




---

ğŸ”„ Azure Data Factory â€“ Pipeline Orchestration

The master pipeline performs:

âœ”ï¸ SQL â†’ ADLS copy
âœ”ï¸ Mapping Data Flow transformation
âœ”ï¸ Databricks notebook execution
âœ”ï¸ Logging into SQL Server
âœ”ï¸ Success/Failure email trigger


---

ğŸ“· Master Pipeline
<img width="1920" height="1080" alt="copy_transform_notebook" src="https://github.com/user-attachments/assets/4b1ead1f-e0f0-48d3-b660-c7f58ff8ecdd" />
<img width="1920" height="1080" alt="log_pipeline_run" src="https://github.com/user-attachments/assets/9181c591-7ef7-4c4c-ac80-7ded7d64642d" />


---

ğŸ“Š Data Transformation â€“ Mapping Data Flow

Transformations Implemented

âœ”ï¸ Data cleaning (null handling, filtering)
âœ”ï¸ Product-wise total sales calculation
âœ”ï¸ Order-level revenue aggregation
âœ”ï¸ Customer-level revenue summary
âœ”ï¸ Margin classification (High / Medium / Low)
âœ”ï¸ Salesperson ranking using window functions
âœ”ï¸ Enrichment joins between Product, Customer & Sales tables


---

ğŸ“· Data Flow Design

<img width="1920" height="1080" alt="mapping_dataflow" src="https://github.com/user-attachments/assets/8113c4ef-cc7a-40c1-9254-8cfe575c4461" />


---

ğŸ’¾ Azure Data Lake Storage (ADLS Gen2)

Two-layer data architecture:

/source_folder   â†’ Raw extracted data
/target_folder   â†’ Transformed Parquet files

âœ”ï¸ Optimized storage using Parquet
âœ”ï¸ Structured folder organization
âœ”ï¸ Secure service principal authentication


---

âš¡ï¸ Azure Databricks â€“ Ingestion Layer

Databricks notebook reads transformed Parquet files from ADLS and loads them into Unity Catalog managed Delta tables.


---

ğŸ“· Databricks Notebook
<img width="1920" height="1080" alt="adb_notebook" src="https://github.com/user-attachments/assets/d13344d4-5cdd-4fa6-b5eb-66c0e21e3886" />



Spark session configuration

Schema creation

Delta table creation

Data load & validation



---

ğŸ—ƒï¸ Unity Catalog â€“ Final Hive Tables

Catalog: adb_adventureworks_etl
Schema: adventureworks_transformed

Tables Created:

customer_orders

order_total

order_details_ext

product_performance

high_margin_products

medium_margin_products

low_margin_products

sales_person_ranked

---

ğŸ“· Unity Catalog Tables

<img width="1920" height="1080" alt="adb_hive_tables" src="https://github.com/user-attachments/assets/89fcc2e9-d58a-4103-a4ea-d495d121ea58" />


---

ğŸ“ˆ Production Logging Framework

Custom logging implemented in SQL Server:

Pipeline_Run

Pipeline_Log

Activity_Details


Logs capture:

Pipeline Name

Run ID

Trigger Type

Start Time

End Time

Execution Duration

Status (Success / Failed)



---

ğŸ“· SQL Logging Tables
<img width="1920" height="1080" alt="sqlServer_logged_pipeline_data" src="https://github.com/user-attachments/assets/71b56f54-6a78-4460-901e-83e9f9fce899" />


---

ğŸ“§ Email Alert System â€“ Logic App Integration
Logic App is triggered via ADF Web Activity.

Email includes:

Pipeline Name

Run ID

Data Factory Name

Execution Status



---

ğŸ“· Logic App Configuration

<img width="1920" height="1080" alt="logic_app" src="https://github.com/user-attachments/assets/f4f74660-f567-4784-9728-9bc59ee4c8c1" />


---

ğŸ” Security Implementation

âœ”ï¸ Service Principal authentication
âœ”ï¸ No hardcoded credentials
âœ”ï¸ Secure Linked Services
âœ”ï¸ Parameterized datasets
âœ”ï¸ Unity Catalog governance


---

ğŸ› ï¸ Technologies Used

Azure Data Factory

Azure Data Lake Storage Gen2

Azure Databricks

Unity Catalog

PySpark

SQL Server

Logic Apps

Delta Lake

Parquet



---

ğŸ¯ Key Data Engineering Concepts Demonstrated

End-to-end ETL orchestration

Layered data architecture

Business rule transformation

Window functions & ranking

Aggregations & joins

Production logging design

Error handling framework

Event-driven email notifications

Secure cloud authentication



ğŸ‘¨â€ğŸ’» Author

Suryansh Tomar
